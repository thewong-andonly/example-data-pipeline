{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Updating the data pipeline\n",
    "\n",
    "This Jupyter Notebook will focus on taking the example data pipeline and applying better coding practices.  The actual code itself and the way the pipeline process information will largely be the same.  \n",
    "\n",
    "This will include, but not be limited to:\n",
    "\n",
    "* Adding upper case text for constants.\n",
    "\n",
    "* Reorganising functions to clearly lay out ETL principles.\n",
    "\n",
    "* Generally make the program more legible.\n",
    "\n",
    "Original code found here: https://github.com/thewong-andonly/example-data-pipeline)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Import tools"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from requests import get\n",
    "import lxml\n",
    "import time\n",
    "import csv\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "## Extract - scrape_data()\n",
    "\n",
    "This is the function doing most of the hard work and forms the \"Extract\" part of the ETL pipeline.  It uses Selenium to navigate through web pages and a mix of Selenium and BeautifulSoup to parse the data."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_data():\n",
    "    \"\"\"\n",
    "    Combination of table 1 and table 2.  Does the following:\n",
    "    \n",
    "    1. Parses the hyperlink from the dynamic main page to give a static page.\n",
    "    \n",
    "    2. Parses static page for relevant information.\n",
    "    \n",
    "    3. Parses main page for relevant information.\n",
    "    \n",
    "    `OUTPUT = True` prints all of the scraped details.  \n",
    "    `OUTPUT = False` turns off verbose mode.\n",
    "    \n",
    "    write:\n",
    "    `WRITE = True` calls `process _data()` function and saves the file to the current working directory to a csv.\n",
    "    `WRITE = False` does not save the file.\n",
    "    \"\"\"\n",
    "    i = 0\n",
    "    table = DRIVER.find_elements_by_xpath(\"//a[@class='pointer']\")\n",
    "    for link in table:\n",
    "        elements = DRIVER.find_elements_by_tag_name('td')\n",
    "        \n",
    "        # Counts the number of columns and number of rows\n",
    "        columns = DRIVER.find_elements_by_tag_name('tr')\n",
    "        number_of_columns = (len(columns[0].text.split()))\n",
    "        number_of_rows = len(elements[::number_of_columns])\n",
    "        \n",
    "        # Filter out language hyperlinks\n",
    "        hyperlink = link.get_attribute(\"href\")\n",
    "        if 'analysis' in hyperlink:\n",
    "           \n",
    "            # Scraping hyperlink data:\n",
    "            \n",
    "            url = get(hyperlink)\n",
    "            soup = BeautifulSoup(url.text, 'html.parser')\n",
    "            table = soup.findAll(\"li\", {\"class\":\"list-group-item\"})\n",
    "\n",
    "            # Profits\n",
    "            pr = soup.findAll(\"div\", {\"class\": \"number\"})\n",
    "            pro = pr[2].text.strip()\n",
    "            profit = pro.encode('ascii', errors='ignore')\n",
    "\n",
    "            # Balance\n",
    "            ba = soup.find(\"li\", class_=\"list-group-item\")\n",
    "            bal = ba.text.strip().split()\n",
    "            balance = bal[1].encode('ascii', errors='ignore')\n",
    "\n",
    "            # Equity\n",
    "            eq = table[1].text.split()\n",
    "            equity = eq[2].encode('ascii', errors='ignore')\n",
    "\n",
    "            # Deposits\n",
    "            de = table[3].text.split()\n",
    "            deposits = de[1].encode('ascii', errors='ignore')\n",
    "\n",
    "            # Withdrawals\n",
    "            wi=table[4].text.strip().split()\n",
    "            withdrawals=wi[1].encode('ascii', errors='ignore')\n",
    "\n",
    "            # Trades\n",
    "            tr = table[5].text.split()\n",
    "            trades = tr[1]\n",
    "\n",
    "            # Won\n",
    "            wo = table[7].text.strip().split()\n",
    "            won = wo[1].encode('ascii', errors='ignore')\n",
    "\n",
    "            # Average trade time\n",
    "            avg_trade_time = table[8].text.strip().split()\n",
    "            att = ' '.join(avg_trade_time[3:5])\n",
    "            \n",
    "            if len(att) == 1:\n",
    "                att_hours = att\n",
    "            if len(att) > 4:\n",
    "                h = att.split('h')\n",
    "                hours = int(h[0])\n",
    "                m = h[1].split('m')\n",
    "                mins = int(m[0])\n",
    "                att_hours = (hours+mins/60)\n",
    "            if len(att) <= 4:\n",
    "                if 'm' in att:\n",
    "                    m = att.split('m')\n",
    "                    mins = int(m[0])\n",
    "                    att_hours = mins/60\n",
    "                if 'd' in att:\n",
    "                    d = att.split('d')\n",
    "                    days= int(d[0])\n",
    "                    att_hours = days*24\n",
    "                \n",
    "            # Profit Factor\n",
    "            pf = table[10].text.strip().split()\n",
    "            profit_factor = pf[2].encode('ascii', errors='ignore')\n",
    "\n",
    "            # Daily\n",
    "            da = table[11].text.strip().split()\n",
    "            daily = da[1].encode('ascii', errors='ignore')\n",
    "\n",
    "            # Monthly\n",
    "            mo = table[12].text.strip().split()\n",
    "            monthly = mo[1].encode('ascii', errors='ignore')\n",
    "\n",
    "            # Trades per month\n",
    "            tpm_ = table[13].text.strip().split()\n",
    "            tpm = tpm_[3].encode('ascii', errors='ignore')\n",
    "\n",
    "            # Expectancy\n",
    "            ex = table[14].text.strip().split()\n",
    "            expectancy = ex[4].encode('ascii', errors='ignore')\n",
    "\n",
    "            t2 = soup.find(\"div\", {\"class\":\"caption-helper font-blue-sharp bold master-description-container\"})\n",
    "            t2_parsed = t2.text.split(', ')\n",
    "\n",
    "            # Rank\n",
    "            if '#' in t2_parsed[0]:\n",
    "                r = t2_parsed[0].split('#')\n",
    "                rank = r[1]\n",
    "            if '#' not in t2_parsed[0]:\n",
    "                rank = '-'\n",
    "\n",
    "            # Platform\n",
    "            platform = t2_parsed[3]\n",
    "\n",
    "            # Ratio\n",
    "            ratio = t2_parsed[4].split(':')\n",
    "            ratio1 = ratio[0]\n",
    "            ratio2 = ratio[1]\n",
    "\n",
    "            # Platform 2\n",
    "            platform2 = t2_parsed[5]\n",
    "\n",
    "            # Info\n",
    "            info_table = soup.find(\"p\")\n",
    "            i_ = info_table.text.strip()\n",
    "            info = i_.encode('ascii', errors='ignore')\n",
    "            \n",
    "            # Scraping search page data:\n",
    "            rank = elements[i].text\n",
    "            name = elements[i+1].text\n",
    "            gain = elements[i+2].text\n",
    "            pips = elements[i+3].text\n",
    "            dd = elements[i+4].text\n",
    "            trades = elements[i+5].text\n",
    "            price = elements[i+8].text\n",
    "            age = elements[i+9].text\n",
    "            added = elements[i+10].text\n",
    "            i += number_of_columns\n",
    "    \n",
    "            # Optional output\n",
    "            if OUTPUT == True:\n",
    "                print(f'Data for {name}.\\nHyperlink: {hyperlink}\\n')\n",
    "                #table 1/main page\n",
    "                print(f'Rank: {rank}')\n",
    "                print(f'Name: {name}') \n",
    "                print(f'Gain: {gain}') \n",
    "                print(f'Pips: {pips}') \n",
    "                print(f'DD: {dd}')\n",
    "                print(f'Trades: {trades}')\n",
    "                print(f'Price: {price} ')\n",
    "                print(f'Age: {age}')\n",
    "                print(f'Added: {added}')\n",
    "\n",
    "                #hyperlink data\n",
    "                print(f'Rank: {rank}')\n",
    "                print(f'Platform: {platform}')\n",
    "                print(f'Ratio: {ratio1}:{ratio2}')\n",
    "                print(f'Platform 2: {platform2}')\n",
    "                print(f\"Profit: {profit}\")\n",
    "                print(f\"Balance: {balance}\")\n",
    "                print(f\"Equity: {equity}\")\n",
    "                print(f\"Deposits: {deposits}\")\n",
    "                print(f\"Withdrawals: {withdrawals}\")\n",
    "                print(f\"Trades: {trades}\")\n",
    "                print(f\"Won %: {won}\")\n",
    "                print(f\"Average trade time: {att}\")\n",
    "                print(f\"Average trade time (hours): {att_hours}\")\n",
    "                print(f\"Profit factor: {profit_factor}\")\n",
    "                print(f'Daily: {daily}')\n",
    "                print(f'Monthly: {monthly}')\n",
    "                print(f'Trades per month: {tpm}')\n",
    "                print(f'Expectancy: {expectancy}')\n",
    "                print(f'Info:{info}\\n')\n",
    "\n",
    "            # Transform - write to CSV file\n",
    "            if WRITE == True:\n",
    "                # All columns for CSV\n",
    "                all_signals_file_columns = [\n",
    "                    'Rank', \n",
    "                    'Name', \n",
    "                    'Hyperlink', \n",
    "                    'Gain', \n",
    "                    'Pips', \n",
    "                    'DD', \n",
    "                    'Trades', \n",
    "                    'Price', \n",
    "                    'Age', \n",
    "                    'Added', \n",
    "                    'Platform', \n",
    "                    'Ratio1', \n",
    "                    'Ratio2', \n",
    "                    'Platform 2', \n",
    "                    'Profit', \n",
    "                    'Balance', \n",
    "                    'Equity', \n",
    "                    'Deposits', \n",
    "                    'Withdrawals', \n",
    "                    'Trades', \n",
    "                    'Won', \n",
    "                    'Average Trade Time', \n",
    "                    'Average Trade time (hours)', \n",
    "                    'Profit Factor',\n",
    "                    'Daily', \n",
    "                    'Monthly', \n",
    "                    'Trades per month', \n",
    "                    'Expectancy']\n",
    "                \n",
    "                all_signals_file_data = [\n",
    "                    [rank, \n",
    "                    name, \n",
    "                    hyperlink, \n",
    "                    gain, \n",
    "                    pips, \n",
    "                    dd, \n",
    "                    trades, \n",
    "                    price, \n",
    "                    age, \n",
    "                    added, \n",
    "                    platform, \n",
    "                    ratio1, \n",
    "                    ratio2, \n",
    "                    platform2, \n",
    "                    profit, \n",
    "                    balance, \n",
    "                    equity, \n",
    "                    deposits, \n",
    "                    withdrawals, \n",
    "                    trades, \n",
    "                    won, \n",
    "                    att, \n",
    "                    att_hours, \n",
    "                    profit_factor, \n",
    "                    daily, \n",
    "                    monthly, \n",
    "                    tpm, \n",
    "                    expectancy]\n",
    "                    ]\n",
    "                signals_info_file_data = [[name, info]]\n",
    "               \n",
    "                signals_info_file_columns = ['Name', 'Info']\n",
    "\n",
    "                # Transform\n",
    "                process_data('all-signals-no-info', all_signals_file_columns, all_signals_file_data, clean=True)\n",
    "                process_data('all-signals-info-only', signals_info_file_columns, signals_info_file_data, clean=False)"
   ]
  },
  {
   "source": [
    "## Transform - process_data()\n",
    "This is used in the \"Transform\" section of the ETL pipeline by converting the extracted data into a .csv format and cleaning it appropriately using a RegEx function and the Pandas library."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(target_filename, column_headers=[], data_fields=[], clean=None):\n",
    "    \"\"\"\n",
    "    Writes data to CSV and returns CSV as raw data\n",
    "    \n",
    "    `column_headers` inputted as as list\n",
    "    `data_fields` inputted as a list of a list\n",
    "    \n",
    "    clean:\n",
    "    `clean = True` passes the target file to the clean_data function which produces columns with digits and punctuation only.\n",
    "    `clean = False` does not pass the target file.\n",
    "    \"\"\"\n",
    "    # Formatting file name\n",
    "    date = datetime.datetime.now().strftime('%d-%m-%Y')\n",
    "    file_name = date+'-'+target_filename+'.csv'\n",
    "    \n",
    "    # Creating data file\n",
    "    raw_data_file_exists = os.path.isfile(file_name)\n",
    "    with open(file_name, 'a') as file:\n",
    "        if not raw_data_file_exists:\n",
    "                # Add header once\n",
    "                fields = column_headers\n",
    "                writer = csv.DictWriter(file, fieldnames=fields)\n",
    "                writer.writeheader()\n",
    "        writer = csv.writer(file)\n",
    "        for d in data_fields:\n",
    "            writer.writerow(d)\n",
    "    \n",
    "    # Creating list of files for cleaning\n",
    "    if clean == True:\n",
    "        global data_to_clean\n",
    "        data_to_clean = []\n",
    "        data_to_clean.append(file_name)\n",
    "        return data_to_clean"
   ]
  },
  {
   "source": [
    "## Transform - clean_data()\n",
    "\n",
    "Runs a RegEx to delete unwanted characters from the signal data.\n",
    "\n",
    "RegEx here: `str.replace(r'[a-zA-Z%$\\'\\+]+', '')`"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(target_filename):\n",
    "    df = pd.read_csv(target_filename)\n",
    "    \n",
    "    # Drop unnamed columns\n",
    "    for c in df.columns:\n",
    "        if 'Unnamed' in c:\n",
    "            df = df.drop(c, axis=1)\n",
    "    \n",
    "    # Strip away all characters which aren't numbers or punctuation for certain columns\n",
    "    num_only_columns = [\n",
    "    'Gain',\n",
    "    'DD',\n",
    "    'Price',\n",
    "    'Profit', \n",
    "    'Balance', \n",
    "    'Equity', \n",
    "    'Deposits', \n",
    "    'Withdrawals', \n",
    "    'Won', \n",
    "    'Profit Factor', \n",
    "    'Daily', \n",
    "    'Monthly', \n",
    "    'Trades per month', \n",
    "    'Expectancy'\n",
    "    ]\n",
    "    \n",
    "    for c in num_only_columns:\n",
    "            df[c] = df[c].str.replace(r'[a-zA-Z%$\\'\\+]+', '')\n",
    "    \n",
    "    # Rename certain columns\n",
    "    df = df.rename({'Gain': 'Gain (%)',\n",
    "                    'DD': 'DD (%)',\n",
    "                    'Price': 'Price ($)',\n",
    "                    'Trades.1': 'Trades',\n",
    "                    'Won': 'Won %',\n",
    "                    'Daily': 'Daily (%)',\n",
    "                    'Monthly': 'Monthly (%)'})\n",
    "    \n",
    "    # Save formatted data\n",
    "    df.to_csv('cleaned-'+target_filename)"
   ]
  },
  {
   "source": [
    "## Extract, Transform - Main program\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Initialise Webdriver\n",
    "URL = # target website\n",
    "DRIVER = webdriver.Chrome()\n",
    "DRIVER.get(URL)\n",
    "#DRIVER.maximize_window()\n",
    "\n",
    "# Program params:\n",
    "\n",
    "# OUTPUT = True, provides print statements of data.\n",
    "# OUTPUT = False, provides no output.\n",
    "OUTPUT = True\n",
    "\n",
    "# WRITE = True, saves the data to a csv.\n",
    "# WRITE = False, doesn't save a file (for testing purposes).\n",
    "WRITE = True\n",
    "\n",
    "# CLEAN = True, uses a RegEx to delete unwanted characters.  Recommended to be True for signal data.\n",
    "# CLEAN = False, does not use the RegEx. Recommended to be true for info only.\n",
    "CLEAN = True\n",
    "\n",
    "# LOAD = True, opens up target file in Pandas.\n",
    "# LOAD = False, doesn't open up file.\n",
    "LOAD = True\n",
    "\n",
    "# Define number of pages\n",
    "page_count = DRIVER.find_element_by_class_name('pagination-panel-total')\n",
    "pages = int(page_count.text)\n",
    "print(f'Scraping {URL} ...')\n",
    "\n",
    "# Set for when loop ends\n",
    "i = 1\n",
    "for num in range(pages+1):\n",
    "    if i > 1:\n",
    "        time.sleep(2)\n",
    "    print(f'Scraping page {i} of {pages} ...\\n')\n",
    "    \n",
    "    # Extract & Transform (webscraping and saving to CSV)\n",
    "    scrape_data()\n",
    "\n",
    "    print(f'Page {i} scraped.\\n')\n",
    "    \n",
    "    # Determine if loop continues\n",
    "    i += 1\n",
    "    if i == pages + 1:\n",
    "        DRIVER.close()\n",
    "        print('All pages scraped!')\n",
    "        break\n",
    "    if i != pages:\n",
    "        time.sleep(1)\n",
    "        WebDriverWait(DRIVER, 10).until(EC.element_to_be_clickable((By.XPATH, \"//a[@class='btn btn-sm default next ']\")))\n",
    "        button = DRIVER.find_element_by_xpath(\"//a[@class='btn btn-sm default next ']\")\n",
    "        button.click()\n",
    "        print(f'Loading page {i}...')\n",
    "        print(f'Page {i} loaded.\\n')\n",
    "\n",
    "# Transform (data cleaning)\n",
    "if CLEAN == True:\n",
    "    print('\\nCleaning data...\\n')\n",
    "    for file in data_to_clean:\n",
    "        clean_data(file)\n",
    "    print(f'{file} cleaned!')\n",
    "\n",
    "# Load   \n",
    "if LOAD == True:\n",
    "    df = pd.read_csv(file)\n",
    "    df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}