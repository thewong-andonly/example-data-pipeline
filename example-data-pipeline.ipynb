{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Return of the Scrape\n",
    "\n",
    "Will try and maintain a LotR theme.  Apologies for any tenuous links.\n",
    "\n",
    "As found in the previous github entry, a previous Upwork client requested I carry out more work for them.  As a refresher, the previous job involved scraping data from hyperlinks in an excel spreadsheet and filling a table full of data.  After I finisheed, I was exceptionally pleased and the following conversation happened with my client:\n",
    "\n",
    "Me: I've done everything, could you please check it's okay? Thank you!\n",
    "Client: Looks great, Mike.  I'd very much like the same but for all signals please.\n",
    "\n",
    "The spreadsheet was full.  I was confused.\n",
    "\n",
    "Me: As far as I can see all the columns are filled.  When you say all signals, could you give me more detail please?\n",
    "Client: Ah, so you see this link here (link to website) and then see all the signals on the left.  I'd like a spreadsheet with all of these please.\n",
    "\n",
    "Yeah, it was a whole website.  But, never fear.  Cups of tea and Google solve everything."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial thoughts\n",
    "\n",
    "So, we already have a few very significant challenges ahead of us from the get go:\n",
    "\n",
    "### The Site\n",
    "* The previous pages the client linked us were static and straightforward to parse.  The client has given us a whole website...which has dynamic webpages in it.\n",
    "\n",
    "* It being dynamic means we can't use BeautifulSoup to our advantage.  We'll have to use Selenium.\n",
    "\n",
    "### The Data\n",
    "\n",
    "* Data consists of a dynamically loaded table which means no clear labels for any of the text.\n",
    "\n",
    "* The website is controlled by left and right arrows with a selection of showing 10/20/30/40/100 records, defaulting on 20.  There's no way of selecting 100 records per page using Selenium.\n",
    "   \n",
    "* This is also a lot more data.\n",
    "\n",
    "### The Approach\n",
    "\n",
    "* Get the first page of data.\n",
    "\n",
    "* Parse all hyperlinks.\n",
    "\n",
    "* Loop through hyperlinks. \n",
    "\n",
    "* Parse data from hyperlinks using previous code.\n",
    "\n",
    "* Write all to an excel file onto 3 separate worksheets:\n",
    "    * Table 1: the main table from the search page.\n",
    "    * Ratios added to a seperate page to make formatting a lot easier later.\n",
    "    * Table 2: the parsed data from the hyperlinks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gathering the Fellowship (Import tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from requests import get\n",
    "import lxml\n",
    "import time\n",
    "import csv\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Elements for Table 1 \n",
    "\n",
    "These are all of the elements found on the search page of the site:\n",
    "\n",
    "* Rank\n",
    "* Name\n",
    "* Gain\n",
    "* Pips\n",
    "* DD\n",
    "* Trades\n",
    "* Type\n",
    "* Price\n",
    "* Age\n",
    "* Added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def table_1(output=False):\n",
    "    \"\"\"\n",
    "    1. Calculates pages to be scraped.\n",
    "    \n",
    "    2. Scrapes desired elements: Rank, Name, Gain, Pips, DD, Trades, Type, Price, Age, Added\n",
    "    \"\"\"\n",
    "    elements = driver.find_elements_by_tag_name('td')\n",
    "    number_of_rows = len(elements[::12])\n",
    "\n",
    "    # Scraping for table 1 elements:\n",
    "    i=0\n",
    "    for num in range(number_of_rows):\n",
    "        rank = elements[i].text\n",
    "        name = elements[i+1].text\n",
    "        gain = elements[i+2].text\n",
    "        pips = elements[i+3].text\n",
    "        dd = elements[i+4].text\n",
    "        trades = elements[i+5].text\n",
    "        type_ = elements[i+6].text\n",
    "        price = elements[i+9].text\n",
    "        age = elements[i+10].text\n",
    "        added = elements[i+11].text\n",
    "        i += 12\n",
    "        \n",
    "        if output == True:\n",
    "            # Optional output\n",
    "            print(f'Table 1 data for {name}:')\n",
    "            print(f'Rank: {rank}')\n",
    "            print(f'Name: {name}') \n",
    "            print(f'Gain: {gain}') \n",
    "            print(f'Pips: {pips}') \n",
    "            print(f'DD: {dd}')\n",
    "            print(f'Trades: {trades}')\n",
    "            print(f'Type: {type_}' )\n",
    "            print(f'Price: {price} ')\n",
    "            print(f'Age: {age}')\n",
    "            print(f'Added: {added}\\n')\n",
    "        if output == False:\n",
    "            pass "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Elements for (the) Table (2 (Towers))\n",
    "\n",
    "Now the search page data has been collected, we now need to collect all of the data within the hyperlinks from the first page.  These are all the desired elements from the hyperlinks:\n",
    "\n",
    "* Profits\n",
    "* Balance\n",
    "* Equity\n",
    "* Deposits\n",
    "* Withdrawals\n",
    "* Trades\n",
    "* Won(%\n",
    "* Average Trade Time\n",
    "    * Calculates average trade time into hours\n",
    "* Profit Factor\n",
    "* Daily \n",
    "* Monthly\n",
    "* Trades per Month\n",
    "* Expectancy\n",
    "* Rank\n",
    "* Platform\n",
    "* Ratio\n",
    "* Platform 2\n",
    "* Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def table_2(output=False): \n",
    "    \"\"\"\n",
    "    1. Parses text for table 1.\n",
    "    \n",
    "    2. Scraped hyperlinks from table 1.\n",
    "    \n",
    "    2. Cycles through hyperlinks from table 1 to create table 2.\n",
    "    \"\"\"\n",
    "    # Looping through hyperlinks from table 1:\n",
    "    table = driver.find_elements_by_xpath(\"//a[@class='pointer']\")\n",
    "    for link in table:\n",
    "        hyperlink = link.get_attribute(\"href\")\n",
    "        if 'analysis' in hyperlink:\n",
    "           \n",
    "            # Creating table 2:\n",
    "            \n",
    "            url = get(hyperlink)\n",
    "            soup = BeautifulSoup(url.text, 'html.parser')\n",
    "            table = soup.findAll(\"li\", {\"class\":\"list-group-item\"})\n",
    "\n",
    "            # Profits\n",
    "            pr = soup.findAll(\"div\", {\"class\": \"number\"})\n",
    "            pro = pr[2].text.strip()\n",
    "            profit = pro.encode('ascii', errors='ignore')\n",
    "\n",
    "            # Balance\n",
    "            ba = soup.find(\"li\", class_=\"list-group-item\")\n",
    "            bal = ba.text.strip().split()\n",
    "            balance = bal[1].encode('ascii', errors='ignore')\n",
    "\n",
    "            # Equity\n",
    "            eq = table[1].text.split()\n",
    "            equity = eq[2].encode('ascii', errors='ignore')\n",
    "\n",
    "            # Deposits\n",
    "            de = table[3].text.split()\n",
    "            deposits = de[1].encode('ascii', errors='ignore')\n",
    "\n",
    "            # Withdrawals\n",
    "            wi=table[4].text.strip().split()\n",
    "            withdrawals=wi[1].encode('ascii', errors='ignore')\n",
    "\n",
    "            # Trades\n",
    "            tr = table[5].text.split()\n",
    "            trades = tr[1]\n",
    "\n",
    "            # Won\n",
    "            wo = table[7].text.strip().split()\n",
    "            won = wo[1].encode('ascii', errors='ignore')\n",
    "\n",
    "            # Average trade time\n",
    "            avg_trade_time = table[8].text.strip().split()\n",
    "            att = ' '.join(avg_trade_time[3:5])\n",
    "\n",
    "            if len(att) > 4:\n",
    "                h = att.split('h')\n",
    "                hours = int(h[0])\n",
    "                m = h[1].split('m')\n",
    "                mins = int(m[0])\n",
    "                total_hours = (hours+mins/60)\n",
    "            if len(att) <= 4:\n",
    "                if 'm' in att:\n",
    "                    m = att.split('m')\n",
    "                    mins = int(m[0])\n",
    "                    total_hours = mins/60\n",
    "                if 'd' in att:\n",
    "                    d = att.split('d')\n",
    "                    days= int(d[0])\n",
    "                    total_hours = days*24\n",
    "\n",
    "            # Profit Factor\n",
    "            pf = table[10].text.strip().split()\n",
    "            profit_factor = pf[2].encode('ascii', errors='ignore')\n",
    "\n",
    "            # Daily\n",
    "            da = table[11].text.strip().split()\n",
    "            daily = da[1].encode('ascii', errors='ignore')\n",
    "\n",
    "            # Monthly\n",
    "            mo = table[12].text.strip().split()\n",
    "            monthly = mo[1].encode('ascii', errors='ignore')\n",
    "\n",
    "            # Trades per month\n",
    "            tpm_ = table[13].text.strip().split()\n",
    "            tpm = tpm_[3].encode('ascii', errors='ignore')\n",
    "\n",
    "            # Expectancy\n",
    "            ex = table[14].text.strip().split()\n",
    "            expectancy = ex[4].encode('ascii', errors='ignore')\n",
    "\n",
    "            t2 = soup.find(\"div\", {\"class\":\"caption-helper font-blue-sharp bold master-description-container\"})\n",
    "            t2_parsed = t2.text.split(', ')\n",
    "\n",
    "            # Rank\n",
    "            if '#' in t2_parsed[0]:\n",
    "                r = t2_parsed[0].split('#')\n",
    "                rank = r[1]\n",
    "            if '#' not in t2_parsed[0]:\n",
    "                rank = '-'\n",
    "\n",
    "            # Platform\n",
    "            platform = t2_parsed[3]\n",
    "\n",
    "            # Ratio\n",
    "            ratio = t2_parsed[4]\n",
    "\n",
    "            # Platform 2\n",
    "            platform2 = t2_parsed[5]\n",
    "\n",
    "            # Info\n",
    "            info_table = soup.find(\"p\")\n",
    "            i_ = info_table.text.strip()\n",
    "            info = i_.encode('ascii', errors='ignore')\n",
    "\n",
    "        # Optional output:\n",
    "            if output == True:\n",
    "                print(f'Hyperlink data from: {hyperlink}')\n",
    "                print(f'Rank: {rank}')\n",
    "                print(f'Platform: {platform}')\n",
    "                print(f'Ratio: {ratio}')\n",
    "                print(f'Platform 2: {platform2}')\n",
    "                print(f\"Profit: {profit}\")\n",
    "                print(f\"Balance: {balance}\")\n",
    "                print(f\"Equity: {equity}\")\n",
    "                print(f\"Deposits: {deposits}\")\n",
    "                print(f\"Withdrawals: {withdrawals}\")\n",
    "                print(f\"Trades: {trades}\")\n",
    "                print(f\"Won %: {won}\")\n",
    "                print(f\"Average trade time: {att}\")\n",
    "                print(f\"Average trade time (hours): {total_hours}\")\n",
    "                print(f\"Profit factor: {profit_factor}\\n\")\n",
    "            if output == False:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `process_data()` function\n",
    "\n",
    "We need a function which can save all of this data to a CSV which can then imported and cleaned using pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(target_filename, column_headers=[], data_fields=[], clean=None):\n",
    "    \"\"\"\n",
    "    Writes data to CSV and returns CSV as raw data\n",
    "    \n",
    "    `column_headers` inputted as as list\n",
    "    `data_fields` inputted as a list of a list\n",
    "    \n",
    "    clean:\n",
    "    `clean = True` passes the target file to the clean_data function which produces columns with digits and punctuation only.\n",
    "    `clean = False` does not pass the target file.\n",
    "    \"\"\"\n",
    "    # Formatting file name\n",
    "    date = datetime.datetime.now().strftime('%d-%m-%Y')\n",
    "    file_name = date+'-'+target_filename+'.csv'\n",
    "    \n",
    "    # Creating data file\n",
    "    raw_data_file_exists = os.path.isfile(file_name)\n",
    "    with open(file_name, 'a') as file:\n",
    "        if not raw_data_file_exists:\n",
    "                # Add header once\n",
    "                fields = column_headers\n",
    "                writer = csv.DictWriter(file, fieldnames=fields)\n",
    "                writer.writeheader()\n",
    "        writer = csv.writer(file)\n",
    "        for d in data_fields:\n",
    "            writer.writerow(d)\n",
    "    \n",
    "    # Creating list of files for cleaning\n",
    "    if clean == True:\n",
    "        global data_to_clean\n",
    "        data_to_clean = []\n",
    "        data_to_clean.append(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `clean_data()` function\n",
    "\n",
    "Now the data has been collected, we need a function which can clean the data as the client has requested numbers only for certain columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(target_filename):\n",
    "    df = pd.read_csv(target_filename)\n",
    "    \n",
    "    # Drop unnamed columns\n",
    "    for c in df.columns:\n",
    "        if 'Unnamed' in c:\n",
    "            df = df.drop(c, axis=1)\n",
    "    \n",
    "    # Strip away all characters which aren't numbers or punctuation for certain columns\n",
    "    num_only_columns = [\n",
    "    'Gain',\n",
    "    'DD',\n",
    "    'Price',\n",
    "    'Profit', \n",
    "    'Balance', \n",
    "    'Equity', \n",
    "    'Deposits', \n",
    "    'Withdrawals', \n",
    "    'Won', \n",
    "    'Profit Factor', \n",
    "    'Daily', \n",
    "    'Monthly', \n",
    "    'Trades per month', \n",
    "    'Expectancy'\n",
    "    ]\n",
    "    \n",
    "    for c in num_only_columns:\n",
    "            df[c] = df[c].str.replace(r'[a-zA-Z%$\\'\\+]+', '')\n",
    "    \n",
    "    # Rename certain columns\n",
    "    df = df.rename({'Gain': 'Gain (%)',\n",
    "                    'DD': 'DD (%)',\n",
    "                    'Price': 'Price ($)',\n",
    "                    'Trades.1': 'Trades',\n",
    "                    'Won': 'Won %',\n",
    "                    'Daily': 'Daily (%)',\n",
    "                    'Monthly': 'Monthly (%)'})\n",
    "    \n",
    "    # Save formatted data\n",
    "    df.to_csv('cleaned-'+target_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `scrape_data()` function\n",
    "\n",
    "A combination of `table_1()` and `table_2()` to make life a lot easier when it comes to creating a pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_data(output=None, write=None):\n",
    "    \"\"\"\n",
    "    Combination of table 1 and table 2.  Does the following:\n",
    "    \n",
    "    1. Parses the hyperlink from the dynamic main page to give a static page.\n",
    "    \n",
    "    2. Parses static page for relevant information.\n",
    "    \n",
    "    3. Parses main page for relevant information.\n",
    "    \n",
    "    4. Produces optional output for whatever reason.  I thought it looked nice.\n",
    "    output:\n",
    "    `output=True` prints all of the scraped details.  \n",
    "    `output=False` turns off verbose mode.\n",
    "    \n",
    "    write:\n",
    "    `write=True` calls `process _data()` function and saves the file to the current working directory to a csv\n",
    "    `write=False` does not save the file.\n",
    "    \"\"\"\n",
    "    i = 0\n",
    "    table = driver.find_elements_by_xpath(\"//a[@class='pointer']\")\n",
    "    for link in table:\n",
    "        elements = driver.find_elements_by_tag_name('td')\n",
    "        \n",
    "        # Counts the number of columns and number of rows\n",
    "        columns = driver.find_elements_by_tag_name('tr')\n",
    "        number_of_columns = (len(columns[0].text.split()))\n",
    "        number_of_rows = len(elements[::number_of_columns])\n",
    "        \n",
    "        # Filter out language hyperlinks\n",
    "        hyperlink = link.get_attribute(\"href\")\n",
    "        if 'analysis' in hyperlink:\n",
    "           \n",
    "            # Scraping hyperlink data:\n",
    "            \n",
    "            url = get(hyperlink)\n",
    "            soup = BeautifulSoup(url.text, 'html.parser')\n",
    "            table = soup.findAll(\"li\", {\"class\":\"list-group-item\"})\n",
    "\n",
    "            # Profits\n",
    "            pr = soup.findAll(\"div\", {\"class\": \"number\"})\n",
    "            pro = pr[2].text.strip()\n",
    "            profit = pro.encode('ascii', errors='ignore')\n",
    "\n",
    "            # Balance\n",
    "            ba = soup.find(\"li\", class_=\"list-group-item\")\n",
    "            bal = ba.text.strip().split()\n",
    "            balance = bal[1].encode('ascii', errors='ignore')\n",
    "\n",
    "            # Equity\n",
    "            eq = table[1].text.split()\n",
    "            equity = eq[2].encode('ascii', errors='ignore')\n",
    "\n",
    "            # Deposits\n",
    "            de = table[3].text.split()\n",
    "            deposits = de[1].encode('ascii', errors='ignore')\n",
    "\n",
    "            # Withdrawals\n",
    "            wi=table[4].text.strip().split()\n",
    "            withdrawals=wi[1].encode('ascii', errors='ignore')\n",
    "\n",
    "            # Trades\n",
    "            tr = table[5].text.split()\n",
    "            trades = tr[1]\n",
    "\n",
    "            # Won\n",
    "            wo = table[7].text.strip().split()\n",
    "            won = wo[1].encode('ascii', errors='ignore')\n",
    "\n",
    "            # Average trade time\n",
    "            avg_trade_time = table[8].text.strip().split()\n",
    "            att = ' '.join(avg_trade_time[3:5])\n",
    "            \n",
    "            if len(att) == 1:\n",
    "                att_hours = att\n",
    "            if len(att) > 4:\n",
    "                h = att.split('h')\n",
    "                hours = int(h[0])\n",
    "                m = h[1].split('m')\n",
    "                mins = int(m[0])\n",
    "                att_hours = (hours+mins/60)\n",
    "            if len(att) <= 4:\n",
    "                if 'm' in att:\n",
    "                    m = att.split('m')\n",
    "                    mins = int(m[0])\n",
    "                    att_hours = mins/60\n",
    "                if 'd' in att:\n",
    "                    d = att.split('d')\n",
    "                    days= int(d[0])\n",
    "                    att_hours = days*24\n",
    "                \n",
    "            # Profit Factor\n",
    "            pf = table[10].text.strip().split()\n",
    "            profit_factor = pf[2].encode('ascii', errors='ignore')\n",
    "\n",
    "            # Daily\n",
    "            da = table[11].text.strip().split()\n",
    "            daily = da[1].encode('ascii', errors='ignore')\n",
    "\n",
    "            # Monthly\n",
    "            mo = table[12].text.strip().split()\n",
    "            monthly = mo[1].encode('ascii', errors='ignore')\n",
    "\n",
    "            # Trades per month\n",
    "            tpm_ = table[13].text.strip().split()\n",
    "            tpm = tpm_[3].encode('ascii', errors='ignore')\n",
    "\n",
    "            # Expectancy\n",
    "            ex = table[14].text.strip().split()\n",
    "            expectancy = ex[4].encode('ascii', errors='ignore')\n",
    "\n",
    "            t2 = soup.find(\"div\", {\"class\":\"caption-helper font-blue-sharp bold master-description-container\"})\n",
    "            t2_parsed = t2.text.split(', ')\n",
    "\n",
    "            # Rank\n",
    "            if '#' in t2_parsed[0]:\n",
    "                r = t2_parsed[0].split('#')\n",
    "                rank = r[1]\n",
    "            if '#' not in t2_parsed[0]:\n",
    "                rank = '-'\n",
    "\n",
    "            # Platform\n",
    "            platform = t2_parsed[3]\n",
    "\n",
    "            # Ratio\n",
    "            ratio = t2_parsed[4].split(':')\n",
    "            ratio1 = ratio[0]\n",
    "            ratio2 = ratio[1]\n",
    "\n",
    "            # Platform 2\n",
    "            platform2 = t2_parsed[5]\n",
    "\n",
    "            # Info\n",
    "            info_table = soup.find(\"p\")\n",
    "            i_ = info_table.text.strip()\n",
    "            info = i_.encode('ascii', errors='ignore')\n",
    "            \n",
    "            # Scraping search page data:\n",
    "            rank = elements[i].text\n",
    "            name = elements[i+1].text\n",
    "            gain = elements[i+2].text\n",
    "            pips = elements[i+3].text\n",
    "            dd = elements[i+4].text\n",
    "            trades = elements[i+5].text\n",
    "            price = elements[i+8].text\n",
    "            age = elements[i+9].text\n",
    "            added = elements[i+10].text\n",
    "            i += number_of_columns\n",
    "    \n",
    "            # Optional output\n",
    "            if output == True:\n",
    "                print(f'Data for {name}.\\nHyperlink: {hyperlink}\\n')\n",
    "                #table 1/main page\n",
    "                print(f'Rank: {rank}')\n",
    "                print(f'Name: {name}') \n",
    "                print(f'Gain: {gain}') \n",
    "                print(f'Pips: {pips}') \n",
    "                print(f'DD: {dd}')\n",
    "                print(f'Trades: {trades}')\n",
    "                print(f'Price: {price} ')\n",
    "                print(f'Age: {age}')\n",
    "                print(f'Added: {added}')\n",
    "\n",
    "                #hyperlink data\n",
    "                print(f'Rank: {rank}')\n",
    "                print(f'Platform: {platform}')\n",
    "                print(f'Ratio: {ratio1}:{ratio2}')\n",
    "                print(f'Platform 2: {platform2}')\n",
    "                print(f\"Profit: {profit}\")\n",
    "                print(f\"Balance: {balance}\")\n",
    "                print(f\"Equity: {equity}\")\n",
    "                print(f\"Deposits: {deposits}\")\n",
    "                print(f\"Withdrawals: {withdrawals}\")\n",
    "                print(f\"Trades: {trades}\")\n",
    "                print(f\"Won %: {won}\")\n",
    "                print(f\"Average trade time: {att}\")\n",
    "                print(f\"Average trade time (hours): {att_hours}\")\n",
    "                print(f\"Profit factor: {profit_factor}\")\n",
    "                print(f'Daily: {daily}')\n",
    "                print(f'Monthly: {monthly}')\n",
    "                print(f'Trades per month: {tpm}')\n",
    "                print(f'Expectancy: {expectancy}')\n",
    "                print(f'Info:{info}\\n')\n",
    "            if output == False:\n",
    "                pass\n",
    "            \n",
    "            # Write to CSV file\n",
    "            if write == True:\n",
    "                all_signals_file_columns = [\n",
    "                    'Rank', \n",
    "                    'Name', \n",
    "                    'Hyperlink', \n",
    "                    'Gain', \n",
    "                    'Pips', \n",
    "                    'DD', \n",
    "                    'Trades', \n",
    "                    'Price', \n",
    "                    'Age', \n",
    "                    'Added', \n",
    "                    'Platform', \n",
    "                    'Ratio1', \n",
    "                    'Ratio2', \n",
    "                    'Platform 2', \n",
    "                    'Profit', \n",
    "                    'Balance', \n",
    "                    'Equity', \n",
    "                    'Deposits', \n",
    "                    'Withdrawals', \n",
    "                    'Trades', \n",
    "                    'Won', \n",
    "                    'Average Trade Time', \n",
    "                    'Average Trade time (hours)', \n",
    "                    'Profit Factor',\n",
    "                    'Daily', \n",
    "                    'Monthly', \n",
    "                    'Trades per month', \n",
    "                    'Expectancy']\n",
    "                \n",
    "                all_signals_file_data = [\n",
    "                    [rank, \n",
    "                    name, \n",
    "                    hyperlink, \n",
    "                    gain, \n",
    "                    pips, \n",
    "                    dd, \n",
    "                    trades, \n",
    "                    price, \n",
    "                    age, \n",
    "                    added, \n",
    "                    platform, \n",
    "                    ratio1, \n",
    "                    ratio2, \n",
    "                    platform2, \n",
    "                    profit, \n",
    "                    balance, \n",
    "                    equity, \n",
    "                    deposits, \n",
    "                    withdrawals, \n",
    "                    trades, \n",
    "                    won, \n",
    "                    att, \n",
    "                    att_hours, \n",
    "                    profit_factor, \n",
    "                    daily, \n",
    "                    monthly, \n",
    "                    tpm, \n",
    "                    expectancy]\n",
    "                    ]\n",
    "                signals_info_file_data = [[name, info]]\n",
    "                signals_info_file_columns = ['Name', 'Info']\n",
    "                process_data('all-signals-no-info', all_signals_file_columns, all_signals_file_data, clean=True)\n",
    "                process_data('all-signals-info-only', signals_info_file_columns, signals_info_file_data, clean=False)\n",
    "                \n",
    "            if write == False:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One Cell to Rule Them All\n",
    "\n",
    "Putting `scrape_data()`, `process_data()` and `clean_data` into a single cell which effectively makes the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Initialise Webdriver\n",
    "url = #website\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(url)\n",
    "driver.maximize_window()\n",
    "\n",
    "# Define number of pages\n",
    "page_count = driver.find_element_by_class_name('pagination-panel-total')\n",
    "pages = int(page_count.text)\n",
    "print(f'Scraping {url} ...')\n",
    "\n",
    "# Scrape data, set for when loop ends\n",
    "i = 1\n",
    "for num in range(pages+1):\n",
    "    if i > 1:\n",
    "        time.sleep(2)\n",
    "    print(f'Scraping page {i} of {pages} ...\\n')\n",
    "    \n",
    "    scrape_data(output=True, write=True)\n",
    "    \n",
    "    print(f'Page {i} scraped.\\n')\n",
    "    i += 1\n",
    "    if i == pages + 1:\n",
    "        driver.close()\n",
    "        print('All pages scraped!')\n",
    "        break\n",
    "    if i != pages:\n",
    "        time.sleep(1)\n",
    "        WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, \"//a[@class='btn btn-sm default next ']\")))\n",
    "        button = driver.find_element_by_xpath(\"//a[@class='btn btn-sm default next ']\")\n",
    "        button.click()\n",
    "        print(f'Loading page {i}...')\n",
    "        \n",
    "        print(f'Page {i} loaded.\\n')\n",
    "\n",
    "# Cleaning data function\n",
    "if len(data_to_clean) > 1:\n",
    "    print('\\nCleaning data...\\n')\n",
    "    for file in data_to_clean:\n",
    "        clean_data(file)\n",
    "    print(f'{file} cleaned!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Changes to Original Approach\n",
    "\n",
    "* Instead of scraping two tables separately, these were combined into a single function which made it easier to save the data to a CSV/excel spreadsheet.\n",
    "\n",
    "* Originally had a convoluted method of writing to text, importing into excel as a CSV file, cleaning in Excel, and then exporting back into pandas.  This was condensed into writing straight to CSV and formatted.\n",
    "\n",
    "* This was then converted into a full pipeline where data can be scraped, cleaned, and exported a single function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenges and How They Were Addressed\n",
    "\n",
    "* So, dynamic pages are a real pain.  As I mentioned before, there's no way of selecting number of records through Selenium, so instead of cycling through 8 pages of 100, I cycled through 20 pages of 39.  The approach was to have the script look for the elements in the same window and once the next page button was clicked, repeat the script.  This worked well, if a little slow.\n",
    "\n",
    "* Clicking the next button requires an explicit wait.  I couldn't get an implicit/explicit wait to work, so I used `time.sleep()` instead aka cheating.\n",
    "\n",
    "* The rest of this was a lot of list comprehension and slicing the correct elements in the correct manner.\n",
    "\n",
    "* Writing a header once to the CSV, embarassingly, took longer than it should have.\n",
    "\n",
    "* Regex cures all random guff that shouldn't be there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How I would improve this code\n",
    "\n",
    "* Add an explicit wait instead of `time.sleep()`.  Struggling to get this going at the mo.\n",
    "\n",
    "* Come up with some better puns.  Maybe cut out puns altogether."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# For testing the length of the first page. number_of_rows\n",
    "url = #website\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(url)\n",
    "driver.maximize_window()\n",
    "\n",
    "elements = driver.find_elements_by_tag_name('td')\n",
    "\n",
    "columns = driver.find_elements_by_tag_name('tr')\n",
    "number_of_columns = (len(columns[0].text.split()))\n",
    "\n",
    "number_of_rows = len(elements[::number_of_columns])\n",
    "# Scraping for table 1 elements:\n",
    "i=0\n",
    "for num in range(number_of_rows):\n",
    "    rank = elements[i].text\n",
    "    name = elements[i+1].text\n",
    "    gain = elements[i+2].text\n",
    "    pips = elements[i+3].text\n",
    "    dd = elements[i+4].text\n",
    "    trades = elements[i+5].text\n",
    "    price = elements[i+8].text\n",
    "    age = elements[i+9].text\n",
    "    added = elements[i+10].text\n",
    "    i += number_of_columns\n",
    "    \n",
    "    # if output == True:\n",
    "        # Optional output\n",
    "    print(f'Table 1 data for {name}:')\n",
    "    print(f'Rank: {rank}')\n",
    "    print(f'Name: {name}') \n",
    "    print(f'Gain: {gain}') \n",
    "    print(f'Pips: {pips}') \n",
    "    print(f'DD: {dd}')\n",
    "    print(f'Trades: {trades}')\n",
    "    print(f'Price: {price} ')\n",
    "    print(f'Age: {age}')\n",
    "    print(f'Added: {added}\\n')\n",
    "driver.quit()\n",
    "\n",
    "      "
   ]
  },
  {
   "source": [
    "This is me checking if I know how to use github. Hello world!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}